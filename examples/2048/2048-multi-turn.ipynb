{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this agent, click *Runtime* and press *Run all*. Make sure you've enabled a free Tesla T4 GPU!\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/images/ART_pill.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/openpipe\"><img src=\"https://github.com/openpipe/art/raw/main/images/Discord_pill.png\" width=\"145\"></a>\n",
    "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/images/Documentation_pill.png\" width=\"125\"></a></a> Join Discord to ask questions + ⭐ <i>Star us on <a href=\"https://github.com/openpipe/art\">Github</a> </i> ⭐\n",
    "</div>\n",
    "\n",
    "To run on your own machine, follow the installation instructions [here](https://art.openpipe.ai).\n",
    "\n",
    "This notebook shows how to train a Qwen 2.5 7B model to play 2048. It will demonstrate how to set up a multi-turn agent, how to train it, and how to evaluate it.\n",
    "\n",
    "Completions will be logged to OpenPipe, and metrics will be logged to Weights & Biases.\n",
    "\n",
    "You will learn how to [set up](#Setup) a simulated environment, how to declare a [model](#Model), how to define a [rollout](#Rollout), and how to run a [training loop](#Training Loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenPipe client initialized\n"
     ]
    }
   ],
   "source": [
    "import art\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "from openpipe.client import OpenPipe\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "op_client = OpenPipe()\n",
    "print(\"OpenPipe client initialized\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "api = art.LocalAPI(wandb_project=\"agent-reinforcement-training\")\n",
    "model = await api.get_or_create_model(\n",
    "    name=\"2048-multi-turn-001\", base_model=\"Qwen/Qwen2.5-7B-Instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import art\n",
    "from art.utils.get_trajectory_messages import get_trajectory_messages\n",
    "import openai\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "\n",
    "from .utils import (\n",
    "    generate_game,\n",
    "    render_board,\n",
    "    apply_agent_move,\n",
    "    check_game_finished,\n",
    "    max_cell_value,\n",
    ")\n",
    "\n",
    "WINNING_VALUE = 512\n",
    "\n",
    "\n",
    "@art.retry(exceptions=(openai.LengthFinishReasonError, requests.ReadTimeout))\n",
    "async def rollout(\n",
    "    client: openai.AsyncOpenAI, iteration: int, is_validation: bool\n",
    ") -> art.Trajectory:\n",
    "\n",
    "    game = generate_game()\n",
    "\n",
    "    move_number = 0\n",
    "\n",
    "    trajectory = art.Trajectory(\n",
    "        messages_and_choices=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an excellent 2048 player. Always choose the move most likely to lead to combine cells to eventually reach the number 2048. Optional moves are 'left', 'right', 'up', 'down'. Return your move as an XML object with a single property 'move', like so: <move>left</move>\",\n",
    "            }\n",
    "        ],\n",
    "        reward=0,\n",
    "        metrics={\"test\": 5},\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "\n",
    "        trajectory.messages_and_choices.append(\n",
    "            {\"role\": \"user\", \"content\": render_board(game)}\n",
    "        )\n",
    "\n",
    "        requested_at = int(time.time() * 1000)\n",
    "        messages = get_trajectory_messages(trajectory)\n",
    "\n",
    "        async def get_completion():\n",
    "            return await client.chat.completions.create(\n",
    "                max_completion_tokens=2048,\n",
    "                messages=messages,\n",
    "                model=model.name,\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            chat_completion = await get_completion()\n",
    "            last_completion = chat_completion\n",
    "        except openai.LengthFinishReasonError as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(\"caught exception generating chat completion\")\n",
    "            print(e)\n",
    "            global failing_trajectory\n",
    "            failing_trajectory = trajectory\n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            op_client.report(\n",
    "                requested_at=requested_at,\n",
    "                received_at=int(time.time() * 1000),\n",
    "                req_payload={\n",
    "                    \"model\": model.name,\n",
    "                    \"messages\": messages,\n",
    "                    \"metadata\": {\n",
    "                        \"game_id\": game[\"id\"],\n",
    "                        \"notebook-id\": \"2048\",\n",
    "                        \"iteration\": str(iteration),\n",
    "                        \"validation\": str(is_validation),\n",
    "                        \"move_number\": str(move_number),\n",
    "                    },\n",
    "                },\n",
    "                resp_payload=chat_completion,\n",
    "                status_code=200,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error reporting to OpenPipe: {e}\")\n",
    "\n",
    "        choice = chat_completion.choices[0]\n",
    "        content = choice.message.content\n",
    "        assert isinstance(content, str)\n",
    "        trajectory.messages_and_choices.append(choice)\n",
    "\n",
    "        try:\n",
    "            apply_agent_move(game, content)\n",
    "            move_number += 1\n",
    "        except ValueError:\n",
    "            trajectory.reward = -1\n",
    "            break\n",
    "\n",
    "        if check_game_finished(game):\n",
    "            max_value = max_cell_value(game)\n",
    "\n",
    "            if max_value < WINNING_VALUE:\n",
    "                # scale reward logarithmically between 0 for 2 and 1 for 2048\n",
    "                trajectory.reward = (math.log(max_value, 2) - 1) / (\n",
    "                    math.log(WINNING_VALUE, 2) - 1\n",
    "                )\n",
    "            else:\n",
    "                # double reward if it wins\n",
    "                trajectory.reward = 2\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        op_client.update_log_metadata(\n",
    "            filters=[\n",
    "                {\n",
    "                    \"field\": \"completionId\",\n",
    "                    \"equals\": last_completion.id,\n",
    "                }\n",
    "            ],\n",
    "            metadata={\n",
    "                \"reward\": str(trajectory.reward),\n",
    "                \"reward_assigned\": \"true\",\n",
    "            },\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating log metadata: {e}\")\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "openai_client = await model.openai_client()\n",
    "for i in range(await model.get_step(), 500):\n",
    "    train_groups = await art.gather_trajectory_groups(\n",
    "        (\n",
    "            art.TrajectoryGroup(\n",
    "                rollout(openai_client, i, is_validation=False) for _ in range(18)\n",
    "            )\n",
    "            for _ in range(1)\n",
    "        ),\n",
    "        pbar_desc=\"gather\",\n",
    "    )\n",
    "    await model.delete_checkpoints()\n",
    "    await model.train(train_groups, config=art.TrainConfig(learning_rate=3e-5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
