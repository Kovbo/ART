{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__main__.InitArgs() got multiple values for keyword argument 'model_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    107\u001b[39m     loftq_config: \u001b[38;5;28mdict\u001b[39m\n\u001b[32m    108\u001b[39m     temporary_location: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[43mget_model_config\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen2.5-14B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModelConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mInitArgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen2.5-14B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mget_model_config\u001b[39m\u001b[34m(base_model, config)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     15\u001b[39m     config = ModelConfig()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m config.init_args = InitArgs(\n\u001b[32m     17\u001b[39m     model_name=base_model,\n\u001b[32m     18\u001b[39m     max_seq_length=\u001b[32m8192\u001b[39m,\n\u001b[32m     19\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# False for LoRA 16bit\u001b[39;00m\n\u001b[32m     20\u001b[39m     fast_inference=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Enable vLLM fast inference\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# vLLM args\u001b[39;00m\n\u001b[32m     22\u001b[39m     disable_log_requests=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     23\u001b[39m     disable_log_stats=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     24\u001b[39m     enable_prefix_caching=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     25\u001b[39m     gpu_memory_utilization=\u001b[32m0.62\u001b[39m,  \u001b[38;5;66;03m# Reduce if out of memory\u001b[39;00m\n\u001b[32m     26\u001b[39m     max_lora_rank=\u001b[32m32\u001b[39m,\n\u001b[32m     27\u001b[39m     num_scheduler_steps=\u001b[32m16\u001b[39m,\n\u001b[32m     28\u001b[39m     use_async=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     29\u001b[39m     **(config.init_args \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m     30\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[31mTypeError\u001b[39m: __main__.InitArgs() got multiple values for keyword argument 'model_name'"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Any, Literal, TYPE_CHECKING, TypedDict\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from trl import GRPOConfig\n",
    "    from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "\n",
    "    from art import types\n",
    "\n",
    "\n",
    "def get_model_config(\n",
    "    base_model: \"types.BaseModel\", config: \"ModelConfig | None\"\n",
    ") -> \"ModelConfig\":\n",
    "    if config is None:\n",
    "        config = ModelConfig()\n",
    "    config.init_args = InitArgs(\n",
    "        model_name=base_model,\n",
    "        max_seq_length=8192,\n",
    "        load_in_4bit=True,  # False for LoRA 16bit\n",
    "        fast_inference=True,  # Enable vLLM fast inference\n",
    "        # vLLM args\n",
    "        disable_log_requests=True,\n",
    "        disable_log_stats=False,\n",
    "        enable_prefix_caching=True,\n",
    "        gpu_memory_utilization=0.62,  # Reduce if out of memory\n",
    "        max_lora_rank=32,\n",
    "        num_scheduler_steps=16,\n",
    "        use_async=True,\n",
    "        **(config.init_args or {}),\n",
    "    )\n",
    "    return config\n",
    "\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    Model configuration.\n",
    "\n",
    "    Args:\n",
    "        init: Arguments for initializing an Unsloth FastLanguageModel.\n",
    "        peft: Arguments for creating an Unsloth PEFT model wrapper.\n",
    "        train: Arguments for training the model.\n",
    "    \"\"\"\n",
    "\n",
    "    init_args: \"InitArgs | None\" = None\n",
    "    peft_args: \"PeftArgs | None\" = None\n",
    "    # train_args: \"GRPOConfig | None\" = None\n",
    "\n",
    "\n",
    "class OpenAIServerConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    Server configuration.\n",
    "\n",
    "    Args:\n",
    "        server_args: Arguments for the vLLM OpenAI-compatible server.\n",
    "        engine_args: Additional vLLM engine arguments for the OpenAI-compatible server.\n",
    "                     Note that since the vLLM engine is initialized with Unsloth,\n",
    "                     these additional arguments will only have an effect if the\n",
    "                     OpenAI-compatible server uses them elsewhere.\n",
    "    \"\"\"\n",
    "\n",
    "    server_args: \"ServerArgs | None\" = None\n",
    "    engine_args: \"AsyncEngineArgs | None\" = None\n",
    "\n",
    "\n",
    "class InitArgs(TypedDict, total=False):\n",
    "    model_name: str\n",
    "    max_seq_length: int\n",
    "    dtype: str | None\n",
    "    load_in_4bit: bool\n",
    "    load_in_8bit: bool\n",
    "    full_finetuning: bool\n",
    "    token: str | None\n",
    "    device_map: str\n",
    "    rope_scaling: dict | None\n",
    "    fix_tokenizer: bool\n",
    "    trust_remote_code: bool\n",
    "    use_gradient_checkpointing: str\n",
    "    resize_model_vocab: int | None\n",
    "    revision: str | None\n",
    "    use_exact_model_name: bool\n",
    "    fast_inference: bool\n",
    "    gpu_memory_utilization: float\n",
    "    float8_kv_cache: bool\n",
    "    random_state: int\n",
    "    max_lora_rank: int\n",
    "    disable_log_requests: bool\n",
    "    disable_log_stats: bool\n",
    "    enable_prefix_caching: bool\n",
    "    num_scheduler_steps: int\n",
    "    use_async: bool\n",
    "\n",
    "\n",
    "class PeftArgs(TypedDict, total=False):\n",
    "    r: int\n",
    "    target_modules: list[str]\n",
    "    lora_alpha: int\n",
    "    lora_dropout: float\n",
    "    bias: str\n",
    "    layers_to_transform: list[int] | None\n",
    "    layers_pattern: str | None\n",
    "    use_gradient_checkpointing: bool | str\n",
    "    random_state: int\n",
    "    max_seq_length: int  # not used anymore\n",
    "    use_rslora: bool\n",
    "    modules_to_save: list[str] | None\n",
    "    init_lora_weights: bool\n",
    "    loftq_config: dict\n",
    "    temporary_location: str\n",
    "\n",
    "get_model_config(\"Qwen/Qwen2.5-14B-Instruct\", ModelConfig(init_args=InitArgs(model_name=\"Qwen/Qwen2.5-14B-Instruct\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ServerConfig(engine_args=EngineArgs(params=['--host', '0.0.0.0', '--port', '8000']))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EngineArgs:\n",
    "    params: list[str]\n",
    "\n",
    "class ServerConfig(BaseModel):\n",
    "    engine_args: EngineArgs\n",
    "\n",
    "\n",
    "ServerConfig.model_validate(ServerConfig(engine_args=EngineArgs(params=[\"--host\", \"0.0.0.0\", \"--port\", \"8000\"])).model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-29 18:20:23 __init__.py:207] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from mp_actors import move_to_child_process\n",
    "import asyncio\n",
    "\n",
    "\n",
    "class Service:\n",
    "    async def load_unsloth(self) -> None:\n",
    "        import unsloth\n",
    "\n",
    "    async def greet(self, name: str, sleep: float) -> str:\n",
    "        await asyncio.sleep(sleep)\n",
    "        return f\"Hello, {name}!\"\n",
    "\n",
    "    def raise_error(self) -> None:\n",
    "        raise ValueError(\"This is a test error\")\n",
    "\n",
    "\n",
    "service = Service()\n",
    "service = move_to_child_process(service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await service.load_unsloth()\n",
    "await service.greet(\"World\", 1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
